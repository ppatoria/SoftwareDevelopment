Table of Contents





Download Safari Books Online apps: Apple iOS | Android | BlackBerry




Entire Site



Entire Site


Titles Only


Short Cuts


Rough Cuts


In Favorites & Folders

































Safari Books Online



Favorites & Folders


All Shared Lists


Bookmarks


Notes & Tags


Reviews


Help




Help



Support Home


Alerts


Top Knowledge Items


Frequently Asked Questions



Pralay Patoria





Pralay Patoria


Account


Tokens and Downloads


Sign Out



Recent Views


Pattern-Oriented Software Architecture: A Pattern Language for Distributed Computing, 4th Volume


Pattern-Oriented Software Architecture, Volume 2, Patterns for Concurrent and Networked Objects


C++ Network Programming, Volume 2: Systematic Reuse with ACE and Frameworks


Pattern Oriented Software Architecture Volume 5: On Patterns and Pattern Languages


ACE Programmer's Guide, The: Practical Design Patterns for Network and Systems Programming












Pattern-Oriented Software Architecture, Volume 2, Patterns for Concurrent and Networked Objects

Add to Favorites (Key: a)Review this Book




































































 



 
 




 



 
 




 



 
 




 



 
 




 



 
 




 



 
 




 



 
 




 



 
 




 



 
 




 



 
 












































































































































































































[x]
< Return to Search Results





Safari Books Online

Create Bookmark (Key: b)


Create Note or Tag (Key: t)


Email This Page (Key: e)


Print



Zoom Out (Key: -)


Zoom In (Key: +)


Toggle to Full Screen (Key: f)


Previous (Key: p)


Next (Key: n)



URL  Show search termsHelp


Chapter 3: Event Handling Patterns > Proactor












Create Bookmark








Proactor


The Proactor architectural pattern allows event-driven applications to efficiently demultiplex and dispatch service requests triggered by the completion of asynchronous operations, to achieve the performance benefits of concurrency without incurring certain of its liabilities.

Example

Consider a networking application that must perform multiple operations simultaneously, such as a high-performance Web server that processes HTTP requests sent from multiple remote Web browsers [HPS99]. When a user wants to download content from a URL four steps occur:


1 The browser establishes a connection to the Web server designated in the URL and then sends it an HTTP GET request. 

2 The Web server receives the browser’s CONNECT indication event, accepts the connection, reads and then parses the request. 

3 The server opens and reads the specified file. 

4 Finally, the server sends the contents of the file back to the Web browser and closes the connection. 

 

One way to implement a Web server is to use a reactive event demultiplexing model in accordance with the Reactor pattern (179). In this design, whenever a Web browser connects to a Web server, a new event handler is created to read, parse, and process the request and transfer the contents of the file back to the browser. This handler is registered with a reactor that coordinates the synchronous demultiplexing and dispatching of each indication event to its associated event handler.

Although a reactive Web server design is straightforward to program, it does not scale up to support many simultaneous users and/or long-duration user requests, because it serializes all HTTP processing at the event demultiplexing layer. As a result, only one GET request can be dispatched and processed iteratively at any given time.

A potentially more scalable way to implement a Web server is to use some form of synchronous multi-threading. In this model a separate server thread processes each browser’s HTTP GET request [HS98]. For example, a new thread can be spawned dynamically for each request, or a pool of threads can be pre-spawned and managed using the Leader/Followers (447) or Half-Sync/Half-Async (423) patterns. In either case each thread performs connection establishment, HTTP request reading, request parsing, and file transfer operations synchronously—that is, server processing operations block until they complete.

Synchronous multi-threading is a common concurrency model. However, problems with efficiency, scalability, programming complexity, and portability may occur, as discussed in the Example section of the Reactor pattern (179).

On operating systems that support asynchronous I/O efficiently, our Web server can therefore invoke operations asynchronously to improve its scalability further. For example, on Windows NT the Web server can be implemented to invoke asynchronous Win32 operations that process externally-generated indication events, such as TCP CONNECT and HTTP GET requests, and transmit requested files to Web browsers asynchronously.

When these asynchronous operations complete, the operating system returns the associated completion events containing their results to the Web server, which processes these events and performs the appropriate actions before returning to its event loop. Building software that achieves the potential performance of this asynchronous event processing model is hard due to the separation in time and space of asynchronous invocations and their subsequent completion events. Thus, asynchronous programming requires a sophisticated yet comprehensible event demultiplexing and dispatching mechanism.

Context

An event-driven application that receives and processes multiple service requests asynchronously.

Problem

The performance of event-driven applications, particularly servers, in a distributed system can often be improved by processing multiple service requests asynchronously. When asynchronous service processing completes, the application must handle the corresponding completion events delivered by the operating system to indicate the end of the asynchronous computations.

For example, an application must demultiplex and dispatch each completion event to an internal component that processes the results of an asynchronous operation. This component can reply to external clients, such as a Web browser client, or to internal clients, such as the Web server component that initiated the asynchronous operation originally. To support this asynchronous computation model effectively requires the resolution of four forces:
•To improve scalability and latency, an application should process multiple completion events simultaneously without allowing long-duration operations to delay other operation processing unduly.
•To maximize throughput, any unnecessary context switching, synchronization, and data movement among CPUs should be avoided, as outlined in the Example section.
•Integrating new or improved services with existing completion event demultiplexing and dispatching mechanisms should require minimal effort.
•Application code should largely be shielded from the complexity of multi-threading and synchronization mechanisms.

Solution

Split application services into two parts: long-duration operations that execute asynchronously and completion handlers that process the results of these operations when they finish. Integrate the demultiplexing of completion events, which are delivered when asynchronous operations finish, with their dispatch to the completion handlers that process them. Decouple these completion event demultiplexing and dispatching mechanisms from the application-specific processing of completion events within completion handlers.

In detail: for every service offered by an application, introduce asynchronous operations that initiate the processing of service requests ‘proactively’ via a handle, together with completion handlers that process completion events containing the results of these asynchronous operations. An asynchronous operation is invoked within an application by an initiator, for example, to accept incoming connection requests from remote applications. It is executed by an asynchronous operation processor. When an operation finishes executing, the asynchronous operation processor inserts a completion event containing that operation’s results into a completion event queue.

This queue is waited on by an asynchronous event demultiplexer called by a proactor. When the asynchronous event demultiplexer removes a completion event from its queue, the proactor demultiplexes and dispatches this event to the application-specific completion handler associated with the asynchronous operation. This completion handler then processes the results of the asynchronous operation, potentially invoking additional asynchronous operations that follow the same chain of activities outlined above.

Structure

The Proactor pattern includes nine participants:

Handles are provided by operating systems to identify entities, such as network connections or open files, that can generate completion events. Completion events are generated either in response to external service requests, such as connection or data requests arriving from remote applications, or in response to operations an application generates internally, such as time-outs or asynchronous I/O system calls.

 Our Web server creates a separate socket handle for each Web browser connection. In Win32 each socket handle is created in ‘overlapped I/O’ mode, which means that operations invoked on the handles run asynchronously. The Windows NT I/O subsystem also generates completion events when asynchronously-executed operations complete.

Asynchronous operations represent potentially long-duration operations that are used in the implementation of services, such as reading and writing data asynchronously via a socket handle. After an asynchronous operation is invoked, it executes without blocking its caller’s thread of control. Thus, the caller can perform other operations. If an operation must wait for the occurrence of an event, such as a connection request generated by a remote application, its execution will be deferred until the event arrives.

 Our proactive Web server invokes the Win32 AcceptEx() operation to accept connections from Web browsers asynchronously. After accepting connections the Web server invokes the Win32 asynchronous ReadFile() and WriteFile() operations to communicate with its connected browsers.

 

A completion handler specifies an interface that consists of one or more hook methods [Pree95] [GHJV95]. These methods represent the set of operations available for processing information returned in the application-specific completion events that are generated when asynchronous operations finish executing.

Concrete completion handlers specialize the completion handler to define a particular application service by implementing the inherited hook method(s). These hook methods process the results contained in the completion events they receive when the asynchronous operations associated with the completion handler finish executing. A concrete completion handler is associated with a handle that it can use to invoke asynchronous operations itself.

For example, a concrete completion handler can itself receive data from an asynchronous read operation it invoked on a handle earlier. When this occurs, the concrete completion handler can process the data it received and then invoke an asynchronous write operation to return the results to its connected remote peer application.

 Our Web server’s two concrete completion handlers—HTTP acceptor and HTTP handler—perform completion processing on the results of asynchronous AcceptEx(), ReadFile(), and WriteFile() operations. The HTTP acceptor is the completion handler for the asynchronous AcceptEx() operation—it creates and connects HTTP handlers in response to connection request events from remote Web browsers. The HTTP handlers then use asynchronous ReadFile() and WriteFile() operations to process subsequent requests from remote Web browsers.

 

Asynchronous operations are invoked on a particular handle and run to completion by an asynchronous operation processor, which is often implemented by an operating system kernel. When an asynchronous operation finishes executing the asynchronous operation processor generates the corresponding completion event. It inserts this event into the completion event queue associated with the handle upon which the operation was invoked. This queue buffers completion events while they wait to be demultiplexed to their associated completion handler.

 

 In our Web server example, the Windows NT operating system is the asynchronous operation processor. Similarly, the completion event queue is a Win32 completion port [Sol98], which is a queue of completion events maintained by the Windows NT kernel on behalf of an application. When an asynchronous operation finishes the Windows NT kernel queues the completion event on the completion port associated with the handle on which the asynchronous operation was originally invoked.

An asynchronous event demultiplexer is a function that waits for completion events to be inserted into a completion event queue when an asynchronous operation has finished executing. The asynchronous event demultiplexer function then removes one or more completion event results from the queue and returns to its caller.

 One asynchronous event demultiplexer in Windows NT is GetQueuedCompletionStatus(). This Win32 function allows event-driven proactive applications to wait up to an application-specified amount of time to retrieve the next available completion event.

A proactor provides an event loop for an application process or thread. In this event loop, a proactor calls an asynchronous event demultiplexer to wait for completion events to occur. When an event arrives the asynchronous event demultiplexer returns. The proactor then demultiplexes the event to its associated completion handler and dispatches the appropriate hook method on the handler to process the results of the completion event.

 

 Our Web server application calls the proactor’s event loop method. This method calls the GetQueuedCompletionStatus() Win32 function, which is an asynchronous event demultiplexer that waits until it can dequeue the next available completion event from the proactor’s completion port. The proactor’s event loop method uses information in the completion event to demultiplex the next event to the appropriate concrete completion handler and dispatch its hook method.

An initiator is an entity local to an application that invokes asynchronous operations on an asynchronous operation processor. The initiator often processes the results of the asynchronous operations it invokes, in which case it also plays the role of a concrete completion handler.

 In our example HTTP acceptors and HTTP handlers play the role of both initiators and concrete completion handlers within the Web server’s internal thread of control. For example, an HTTP acceptor invokes AcceptEx() operations that accept connection indication events asynchronously from remote Web browsers. When a connection indication event occurs, an HTTP acceptor creates an HTTP handler, which then invokes an asynchronous ReadFile() operation to retrieve and process HTTP GET requests from a connected Web browser.

 

Note how in the Proactor pattern the application components, represented by initiators and concrete completion handlers, are proactive entities. They instigate the control and data flow within an application by invoking asynchronous operations proactively on an asynchronous operation processor.

When these asynchronous operations complete, the asynchronous operation processor and proactor collaborate via a completion event queue. They use this queue to demultiplex the resulting completion events back to their associated concrete completion handlers and dispatch these handlers’ hook methods. After processing a completion event, a completion handler may invoke new asynchronous operations proactively.

The structure of the participants in the Proactor pattern is illustrated in the following class diagram:

 

Dynamics

The following collaborations occur in the Proactor pattern:
•An application component playing the role of an initiator invokes an asynchronous operation on an asynchronous operation processor via a particular handle. In addition to passing data parameters to the asynchronous operation, the initiator also passes certain completion processing parameters, such as the completion handler or a handle to the completion event queue. The asynchronous operation processor stores these parameters internally for later use. 
 The HTTP handler in our Web server can instruct the operating system to read a new HTTP GET request by invoking the ReadFile() operation asynchronously on a particular socket handle. When initiating this operation on the handle, the HTTP handler passes itself as the completion handler so that it can process the results of an asynchronous operation.

•After an initiator invokes an operation on the asynchronous operation processor, the operation and initiator can run independently. In particular, the initiator can invoke new asynchronous operations while others continue to execute concurrently.4 If the asynchronous operation is intended to receive a service request from a remote application, the asynchronous operation processor defers the operation until this request arrives. When the event corresponding to the expected request arrives, the asynchronous operation will finish executing. 
 The Windows NT operating system defers the asynchronous ReadFile() operation used to read an HTTP GET request until this request arrives from a remote Web browser.

•When an asynchronous operation finishes executing, the asynchronous operation processor generates a completion event. This event contains the results of the asynchronous operation. The asynchronous operation processor then inserts this event into the completion event queue associated with the handle upon with the asynchronous operation was originally invoked. 
 If an HTTP handler invoked an asynchronous ReadFile() operation to read an HTTP GET request, the Windows NT operating system will report the completion status in the completion event, such as its success or failure and the number of bytes read.

•When an application is ready to process the completion events resulting from its asynchronous operations, it invokes the proactor’s event loop entry-point method, which we call handle_events(). This method calls an asynchronous event demultiplexer5 to wait on its completion event queue for completion events to be inserted by the asynchronous operation processor. After removing a completion event from the queue the proactor’s handle_events() method demultiplexes the event to its associated completion handler. It then dispatches the appropriate hook method on the completion handler, passing it the results of the asynchronous operation. 
 The proactor in our Web server example uses a Win32 completion port as its completion event queue. Similarly, it uses the Win32 GetQueuedCompletionStatus() function [Sol98] as its asynchronous event demultiplexer to remove completion events from a completion port.

•The concrete completion handler then processes the completion results it receives. If the completion handler returns a result to its caller, two situations are possible. First, the completion handler that processes the results of the asynchronous operations also can be the initiator that invoked the operation originally. In this case the completion handler need not perform additional work to return the result to its caller, because it is the caller. 
Second, a remote application or an application internal component may have requested the asynchronous operation. In this case, the completion handler can invoke an asynchronous write operation on its transport handle to return results to the remote application.

 In response to an HTTP GET request from a remote Web browser, an HTTP handler might instruct the Windows NT operating system to transmit a large file across a network by calling WriteFile() asynchronously. After the operating system completes the asynchronous operation successfully the resulting completion event indicates the number of bytes transferred to the HTTP handler. The entire file may not be transferred in one WriteFile() operation due to transport-layer flow control. In this case the HTTP handler can invoke another asynchronous WriteFile() operation at the appropriate file offset.

•After the completion handler finishes its processing it can invoke other asynchronous operations, in which case the whole cycle outlined in this section begins again.

 

Implementation

The participants in the Proactor pattern can be decomposed into two layers:
•Demultiplexing/dispatching infrastructure layer components. This layer performs generic, application-independent strategies for executing asynchronous operations. It also demultiplexes and dispatches completion events from these asynchronous operations to their associated completion handlers.
•Application layer components. This layer defines asynchronous operations and concrete completion handlers that perform application-specific service processing.

The implementation activities in this section start with the generic demultiplexing/dispatching infrastructure components and then cover the application components. We focus on a proactor implementation that is designed to invoke asynchronous operations and dispatch hook methods on their associated completion handlers using a single thread of control. The Variants section describes the activities associated with developing multi-threaded proactor implementations.


1 Separate application services into asynchronous operations and completion handlers. To implement the Proactor pattern, application services must be designed to separate the initiation of asynchronous operations via a handle from the processing of these operations’ results. Asynchronous operations are often long-duration and/or concerned with I/O, such as reading and writing data via a socket handle or communicating with a database. The results of asynchronous operations are processed by completion handlers. In addition to processing results, completion handlers can play the role of initiators, that is, they invoke asynchronous operations themselves. 

The products of this activity are a set of asynchronous operations, a set of completion handlers, and a set of associations between each asynchronous operation and its completion handler.


2 Define the completion handler interface. Completion handlers specify an interface consisting of one or more hook methods [Pree95]. These hook methods represent the completion handling for application-specific completion events generated when asynchronous operations finish executing. The implementation of completion handlers consists of three sub-activities: 

2.1 Define a type to convey the results of asynchronous operation. When an asynchronous operation completes or is canceled its completion event results must be conveyed to its completion handler. These results indicate its success or failure and the number of bytes that were transmitted successfully. The Adapter pattern [GoF95] is often used to convert information stored in a completion event into a form used to dispatch to its associated concrete completion handler. 

 The following C++ class conveys the results of an asynchronous Win32 operation back to a concrete completion handler:
class Async_Result : public OVERLAPPED {
   // The Win32 OVERLAPPED struct stores the file offset
   // returned when an asynchronous operation completes.
public:
   // Dispatch to completion handler hook method.
   virtual void complete () = 0;
   // Set/get number of bytes transferred by an
   // asynchronous operation.
   void bytes_transferred (u_long);
   u_long bytes_transferred () const;
 
   // Set/get the status of the asynchronous operation,
   // i.e., whether it succeeded or failed.
   void status (u_long);
   u_long status () const;
 
   // Set/get error value if the asynchronous operation
   // failed or was canceled by the initiator.
   void error (u_long);
   u_long error () const;
private:
   // … data members omitted for brevity …
};


Deriving Async_Result from the OVERLAPPED struct allows applications to add custom state and methods to the results of asynchronous operations. C++ inheritance is used because the Win32 API does not provide a more direct way to pass a per-operation result object to the operating system when an asynchronous operation is invoked.


2.2 Determine the type of the dispatching target. Two types of completion handlers can be associated with a handle to serve as the target of a proactor’s dispatching mechanism, objects and pointers to functions. Implementations of the Proactor pattern can choose the type of dispatching target based on the same criteria described in implementation activity 1.1 of the Reactor (179) pattern. 

2.3 Define the completion handler dispatch interface strategy. We next define the type of interface supported by the completion handler to process completion events. As with the Reactor pattern (179), assuming that we use completion handler objects rather than pointers to functions, two general strategies exist: 
•Single-method dispatch interface strategy. The class diagram in the Structure section illustrates an implementation of the Completion_Handler interface that contains a single event handling method, which we call handle_event(). A proactor uses this method to dispatch completion events to their associated completion handlers. In this case the type of completion event that has occurred is passed as a parameter to the method. The second parameter is the base class for all asynchronous results, which, depending on the completion event, can be further downcast to the correct type.

 The following C++ abstract base class illustrates the single-method dispatch interface strategy. We start by defining useful type definitions and enumeration literals that can be used by both the single-method and multi-method dispatch interface strategies: 
typedef unsigned int Event_Type;
enum {
   // Types of indication events.
   READ_EVENT = 01,
   ACCEPT_EVENT = 01, // An “alias” for READ_EVENT.
   WRITE_EVENT = 02, TIMEOUT_EVENT = 04,
   SIGNAL_EVENT = 010, CLOSE_EVENT = 020
   // These values are powers of two so
   // their bits can be “or’d” together efficiently.
};


Next, we implement the Completion_Handler class: 
class Completion_Handler {
public:
   // Cache the <proactor> so that hook methods can
   // invoke asynchronous operations on <proactor>.
   Completion_Handler (Proactor *proactor):
      proactor_ (proactor) { }
 
   // Virtual destruction.
   virtual ~Completion_Handler ();
 
   // Hook method dispatched by cached <proactor_> to
   // handle completion events of a particular type that
   // occur on the <handle>. <Async_Result> reports the
   // results of the completed asynchronous operation.
   virtual void handle_event
      (HANDLE handle, Event_Type et,
      const Async_Result &result) = 0;
 
   // Returns underlying I/O <HANDLE>.
   virtual HANDLE get_handle () const = 0;
private:
   // Cached <Proactor>.
   Proactor *proactor_;
};


The single-method dispatch interface strategy makes it possible to add new types of events without changing the class interface. However, to handle a specific event, this strategy encourages the use of C++ switch and if statements in the concrete event handler’s handle_event() method implementation, which degrades its internal extensibility. 
•Multi-method dispatch interface strategy. A different strategy for implementing the Completion_Handler interface is to define separate hook methods for handling each type of event, such as handle_read(), handle_write(), or handle_accept(). This strategy can be more extensible than the single-method dispatch interface because the demultiplexing is performed by a proactor implementation, rather than by a concrete event handler’s handle_event() method implementation.

 The following C++ abstract base class illustrates a multi-method interface used by a proactor for network events in our Windows NT-based Web server example: 
class Completion_Handler {
public:
   // The <proactor> is cached to allow hook methods to
   // invoke asynchronous operations on <proactor>.
   Completion_Handler (Proactor *proactor):
      proactor_ (proactor) { }
 
   // Virtual destruction.
   virtual ~Completion_Handler ();
 
   // The next 3 methods use <Async_Result> to report
   // results of completed asynchronous operation.
   // Dispatched by <proactor_> when an asynchronous
   // read operation completes.
   virtual void handle_read
      (HANDLE handle, const Async_Result &result) = 0;
   // Dispatched by <proactor_> when an asynchronous
   // write operation completes.
   virtual void handle_write
      (HANDLE handle, const Async_Result &result) = 0;
   // Dispached by <proactor_> when an asynchronous
   // <accept> operation completes.
   virtual void handle_accept
      (HANDLE handle, const Async_Result &result) = 0;
 
   // Dispatched by <proactor_> when a timeout expires.
   virtual void handle_timeout
      (const Time_Value &tv, const void *act) = 0;
 
   // Returns underlying I/O <HANDLE>.
   virtual HANDLE get_handle () const = 0;
private:
   // Cached <Proactor>.
   Proactor *proactor_;
};


The multi-method dispatch interface strategy makes it easy to override methods in the base class selectively, which avoids further demultiplexing via switch or if statements in the hook method implementation. However, this strategy requires pattern implementors to anticipate the hook methods in advance. The various handle_*() hook methods in the Completion_Handler interface above are tailored for networking events. However, these methods do not encompass all the types of events handled via the Win32 WaitForMultipleObjects() mechanism, such as synchronization object events [SchSt95]. 

Both the single-method and multiple-method dispatch interface strategies are implementations of the Hook Method [Pree95] and Template Method [GoF95] patterns. The intent of these patterns is to provide well-defined hooks that can be specialized by applications and called back by lower-level dispatching code.

Completion handlers are often designed to act both as a target of a proactor’s completion dispatching and an initiator that invokes asynchronous operations, as shown by the HTTP_Handler class in the Example Resolved section. Therefore, the constructor of class Completion_Handler associates a Completion_Handler object with a pointer to a proactor. This design allows a Completion_Handler’s hook methods to invoke new asynchronous operations whose completion processing will be dispatched ultimately by the same proactor.


3 Implement the asynchronous operation processor. An asynchronous operation processor executes operations asynchronously on behalf of initiators. Its primary responsibilities therefore include: 
•Defining the asynchronous operation interface
•Implementing a mechanism to execute operations asynchronously and generating and
•Queueing completion events when an operation finishes


3.1 Define the asynchronous operation interface. Asynchronous operations can be passed various parameters, such as a handle,6 data buffers, buffer lengths, and information used to perform completion processing when the operation finishes. Two issues must be addressed when designing a programming interface that initiators use to invoke asynchronous operations on an asynchronous operation processor: 
•Maximizing portability and flexibility. Asynchronous operations can be used to read and write data on multiple types of I/O devices, such as networks and files, and on multiple operating systems, such as Windows NT, VMS, Solaris, and Linux. The Wrapper Facade (47) and Bridge [GoF95] patterns can be applied to decouple the asynchronous operation interface from underlying operating system dependencies and ensure the interface works for multiple types of I/O devices.
•Handling multiple completion handlers, proactors, and completion event queues efficiently and concisely. More than one completion handler, proactor, and completion event queue can be used simultaneously within an application. For example, different proactors can be associated with threads running at different priorities, to provide different quality of service levels for processing different completion handlers. In addition to its data parameters, an asynchronous operation must then indicate which handle, concrete completion handler, proactor, and completion event queue to use when processing the completion of asynchronous operations. 
A common strategy to consolidate all this completion processing information efficiently is to apply the Asynchronous Completion Token pattern (261). When an initiator invokes an asynchronous operation on a handle, an asynchronous completion token (ACT) can then be passed to the asynchronous operation processor, which can store this ACT for later use. Each ACT contains information that identifies a particular operation and guides its subsequent completion processing.

When an asynchronous operation finishes executing, the asynchronous operation processor locates the operation’s ACT it stored earlier and associates it with the completion event it generates. It then inserts this updated completion event into the appropriate completion event queue. Ultimately, the proactor that runs the application’s event loop will use an asynchronous event demultiplexer to remove the completion event results and ACT from its completion event queue. The proactor will then use this ACT to complete its demultiplexing and dispatching of the completion event results to the completion handler designated by the ACT.


 Although our Web server is implemented using Win32 asynchronous Socket operations, we apply the Wrapper Facade pattern (47) to generalize this class and make it platform-independent. It can therefore be used for other types of I/O devices supported by an asynchronous operation processor.

The following Async_Stream class interface is used by HTTP handlers in our Web server example to invoke asynchronous operations:
class Async_Stream {
public:
   // Constructor ‘zeros out’ the data members.
   Async_Stream ();
 
   // Initialization method.
   void open (Completion_Handler *handler,
            HANDLE handle, Proactor *proactor);
 
   // Invoke an asynchronous read operation.
   void async_read (void *buf, u_long n_bytes);
 
   // Invoke an asynchronous write operation.
   void async_write (const void *buf, u_long n_bytes);
private:
   // Cache parameters passed in <open>.
   Completion_Handler *completion_handler_;
   HANDLE handle_;
   Proactor *proactor_;
};


A concrete completion handler, such as an HTTP handler, can pass itself to open(), together with the handle on which the Async_Stream’s async_read() and async_write() methods are invoked:
void Async_Stream::open (Completion_Handler *handler,
                     HANDLE handle,
                     Proactor *proactor) {
   completion_handler_ = handler;
   handle_ = handle;
   proactor_ = proactor;
 
   // Associate handle with <proactor>’s completion
   // port, as shown in implementation activity 4.
   proactor->register_handle (handle);
}


To illustrate the use of asynchronous completion tokens (ACTs), consider the following implementation of the Async_Stream::async_read() method. It uses the Win32 ReadFile() function to read up to n_bytes asynchronously and store them in its buf parameter:
void Async_Stream::read (void *buf, u_long n_bytes) {
   u_long bytes_read;
 
   OVERLAPPED *act = new // Create the ACT.
      Async_Stream_Read_Result (completion_handler_);
 
   ReadFile (handle_, buf, n_bytes, &bytes_read, act);
}


The ACT passed as a pointer to ReadFile() is a dynamically allocated instance of the Async_Stream_Read_Result class below:
class Async_Stream_Read_Result : public Async_Result {
public:
   // Constructor caches the completion handler.
   Async_Stream_Read_Result
      (Completion_Handler *completion_handler):
      completion_handler_ (completion_handler) { }
 
   // Adapter that dispatches the <handle_event>
   // hook method on cached completion handler.
   virtual void complete ();
private:
   // Cache a pointer to a completion handler.
   Completion_Handler *completion_handler_;
};


This class plays the role of an ACT and an adapter [GoF95]. It inherits from Async_Result, which itself inherits from the Win32 OVERLAPPED struct, as shown in implementation activity 2.1 (227). The ACT can be passed as the lpOverlapped parameter to the ReadFile() asynchronous function. ReadFile() forwards the ACT to the Windows NT operating system, which stores it for later use.

When the asynchronous ReadFile() operation finishes it generates a completion event that contains the ACT it received when this operation was invoked. When the proactor’s handle_events() method removes this event from its completion event queue, it invokes the complete() method on the Async_Stream_Read_Result. This adapter method then dispatches the completion handler’s handle_event() hook method to pass the event, as shown in implementation activity 5.4 (240).


3.2 Choose the asynchronous operation processing mechanism. When an initiator invokes an asynchronous operation, an asynchronous operation processor executes the operation without blocking the initiator’s thread of control. An asynchronous operation processor provides mechanisms for managing ACTs and executing operations asynchronously. It also generates completion events when operations finish and queues the events into the appropriate completion event queue. 

Some asynchronous operation processors allow initiators to cancel asynchronous operations. However, completion events are still generated. Thus, ACTs and other resources can be reclaimed properly by completion handlers.

Certain operating environments provide these asynchronous operation execution and completion event generation mechanisms, such as Real-time POSIX [POSIX95] and Windows NT [Sol98]. In this case implementing the asynchronous completion processor participant simply requires mapping existing operating system APIs onto the asynchronous operation wrapper facade (47) interfaces described in implementation activity 3.1 (232). The Variants section describes techniques for emulating an asynchronous operation processor on operating system platforms that do not support this feature natively.


4 Define the proactor interface. The proactor’s interface is used by applications to invoke an event loop that removes completion events from a completion event queue, demultiplexes them to their designated completion handlers, and dispatches their associated hook method. The proactor interface is often accessed via a singleton [GoF95] because a single proactor is often sufficient for each application process. 

The Proactor pattern can use the Bridge pattern [GoF95] to shield applications from complex and non-portable completion event demultiplexing and dispatching mechanisms. The proactor interface corresponds to the abstraction participant in the Bridge pattern, whereas a platform-specific proactor instance is accessed internally via a pointer, in accordance with the implementation hierarchy in the Bridge pattern.

 The proactor interface in our Web server defines an abstraction for associating handles with completion ports and running the application’s event loop proactively:
class Proactor {
public:
   // Associate <handle> with the <Proactor>’s
   // completion event queue.
   void register_handle (HANDLE handle);
 
   // Entry point into the proactive event loop. The
   // <timeout> can bound time waiting for events.
   void handle_events (Time_Value *wait_time = 0);
 
   // Define a singleton access point.
   static Proactor *instance ();
private:
   // Use the Bridge pattern to hold a pointer to
   // the <Proactor_Implementation>.
   Proactor_Implementation *proactor_impl_;
};


A proactor interface also defines a method, which we call register_handle(), that associates a handle with the proactors completion event queue, as described in implementation activity 5.5 (240). This association ensures that the completion events generated when asynchronous operations finish executing will be inserted into a particular proactor’s completion event queue.

The proactor interface also defines the main entry point method, we call it handle_events(), that applications use to run their proactive event loop.7 This method calls the asynchronous event demultiplexer, which waits for completion events to arrive on its completion event queue, as discussed in implementation activity 3.1 (232). An application can use the timeout parameter to bound the time it spends waiting for completion events. Thus, the application need not block indefinitely if events never arrive.

After the asynchronous operation processor inserts a completion event into the proactor’s completion event queue, the asynchronous event demultiplexer function returns. At this point the proactor’s handle_events() method dequeues the completion event and uses its associated ACT to demultiplex to the asynchronous operation’s completion handler and dispatch the handler’s hook method.


5 Implement the proactor interface. Five sub-activities can be used to implement the proactor interface: 

5.1 Develop a proactor implementation hierarchy. The proactor interface abstraction illustrated in implementation activity 4 (235) delegates all its demultiplexing and dispatching processing to a proactor implementation. This plays the role of the implementation hierarchy in the Bridge pattern [GoF95]. This design allows multiple types of proactors to be implemented and configured transparently. For example, a concrete proactor implementation can be created using different types of asynchronous event demultiplexers, such as POSIX aio_suspend() [POSIX95], or the Win32 GetQueuedCompletionStatus() or WaitForMultipleObjects() functions [Sol98]. 

 In our example the base class of the proactor implementation hierarchy is defined by the class Proactor_Implementation. We omit its declaration here because this class has essentially the same interface as the Proactor interface in implementation activity 4 (235). The primary difference is that its methods are purely virtual, because it forms the base of a hierarchy of concrete proactor implementations.


5.2 Choose the completion event queue and asynchronous event demultiplexer mechanisms. The handle_events() method of the proactor implementation calls an asynchronous event demultiplexer function, which waits on the completion event queue for the asynchronous operation processor to insert completion events. This function returns whenever there is a completion event in the queue. Asynchronous event demultiplexers can be distinguished by the types of semantics they support, which include one of the following: 
•FIFO demultiplexing. This type of asynchronous event demultiplexer function waits for completion events corresponding to any asynchronous operations that are associated with its completion event queue. The events are removed from the queue in the order in which they are inserted. 
 The Win32 GetQueuedCompletionStatus() function allows event-driven proactive applications to wait up to an application-specified amount of time for any completion events to occur on a completion port. Events are removed in FIFO order [Sol98].

•Selective demultiplexing. This type of asynchronous event demultiplexer function waits selectively for a particular subset of completion events that must be passed explicitly when the function is called. 
 The POSIX aio_suspend() function [POSIX95] and the Win32 WaitForMultipleObjects() function [Sol98] are passed an array parameter designating asynchronous operations explicitly. They suspend their callers for an application-specified amount of time until at least one of these asynchronous operations has completed.


The completion event queue and asynchronous event demultiplexer are often existing operating system mechanisms that need not be developed by Proactor pattern implementors.

The primary difference between GetQueuedCompletionStatus(), aio_suspend(), and WaitForMultipleObjects() is that the latter two functions can wait selectively for completion events specified via an array parameter. Conversely, GetQueuedCompletionStatus() just waits for the next completion event enqueued on its completion port. Moreover, the POSIX aio_*() functions can only demultiplex asynchronous I/O operations, such as aio_read() or aio_write(), whereas GetQueuedCompletionStatus() and WaitForMultipleObjects() can demultiplex other Win32 asynchronous operations, such as timers and synchronization objects.

 Our Web server uses a Win32 completion port as the completion event queue and the GetQueuedCompletionStatus() function as its asynchronous event demultiplexer:
BOOL GetQueuedCompletionStatus
   (HANDLE CompletionPort,
   LPDWORD lpNumberOfBytesTransferred,
   LPDWORD lpCompletionKey,
   LPOVERLAPPED *lpOverlapped,
   DWORD dwMilliseconds);


As shown in implementation activity 5.5 (240), our proactor implementation’s handle_events() method uses this function to dequeue a completion event from the specified CompletionPort. The number of bytes transferred is returned as an ‘out’ parameter. The lpOverlapped parameter points to the ACT passed by the original asynchronous operation, such as the ReadFile() call in the Async_Stream::async_read() method shown in implementation activity 3.1 (232).

If there are no completion event results queued on the port, the function blocks the calling thread, waiting for asynchronous operations associated with the completion port to finish. The GetQueuedCompletionStatus() function returns when it is able to dequeue a completion event result or when the dwMilliseconds timeout expires.


5.3 Determine how to demultiplex completion events to completion handlers. An efficient and concise strategy for demultiplexing completion events to completion handlers is to use the Asynchronous Completion Token pattern (261), as described in implementation activity 3.1 (232). In this strategy, when an asynchronous operation is invoked by an initiator the asynchronous operation processor is passed information used to guide subsequent completion processing. For example, a handle can be passed to identify a particular socket endpoint and completion event queue, and an ACT can be passed to identify a particular completion handler. 

When the asynchronous operation completes, the asynchronous operation processor generates the corresponding completion event, associates it with its ACT and inserts the updated completion event into the appropriate completion event queue. After an asynchronous event demultiplexer removes the completion event from its completion event queue, the proactor implementation can use the completion event’s ACT to demultiplex to the designated completion handler in constant O(1) time.

 As shown in implementation activity 3.1 (232), when an async_read() or async_write() method is invoked on an Async_Stream, they create a new Async_Stream_Read_Result or Async_Stream_Write_Result ACT, respectively and pass it to the corresponding Win32 asynchronous operation. When this asynchronous operation finishes, the Windows NT kernel queues the completion event on the completion port designated by the handle that was passed during the original asynchronous operation invocation. The ACT is used by the proactor to demultiplex the completion event to the completion handler designated in the original call.


5.4 Determine how to dispatch the hook method on the designated completion handler. After the proactor’s handle_events() method demultiplexes to the completion handler it must dispatch the appropriate hook method on the completion handler. An efficient strategy for performing this dispatching operation is to combine the Adapter pattern [GoF95] with the Asynchronous Completion Token pattern (261), as shown at the end of implementation activity 3.1 (232). 

 An Async_Stream_Read_Result is an adapter, whose complete() method can dispatch the appropriate hook method on the completion handler that it has cached in the state of its ACT:
void Async_Stream_Read_Result::complete () {
   completion_handler_->handle_event
      (completion_handler_->get_handle (),
      READ_EVENT, *this);
}


Note how the handle_event() dispatch hook method is passed a reference to the Async_Stream_Read_Result object that invoked it. This double-dispatching interaction [GoF95] allows the completion handler to access the asynchronous operation results, such as the number of bytes transferred and its success or failure status.


5.5 Define the concrete proactor implementation. The proactor interface holds a pointer to a concrete proactor implementation and forwards all method calls to it, as shown in implementation activity 4 (235). 

 Our concrete proactor implementation overrides the pure virtual methods it inherits from class Proactor_Implementation:
class Win32_Proactor_Implementation :
   public Proactor_Implementation {
public:


The Win32_Proactor_Implementation constructor creates the completion port and caches it in the completion_port_ data member:
   Win32_Proactor_Implementation::
      Win32_Proactor_Implementation () {
         completion_port_ = CreateIoCompletionPort
            (INVALID_HANDLE, 0, 0, 0);
   }


The register_handle() method associates a HANDLE with the completion port:
   void Win32_Proactor_Implementation::register_handle
      (HANDLE h) {
      CreateIoCompletionPort (h, completion_port_,0,0);
   }


All subsequent completion events hat result from asynchronous operations invoked via the HANDLE will be inserted into this proactor’s completion port by the Windows NT operating system.

The next code fragment shows how to implement the handle_events() method:
   void Win32_Proactor_Implementation::handle_events
      (Time_Value *wait_time = 0) {
      u_long num_bytes;
      OVERLAPPED *act;


This method first calls the GetQueuedCompletionStatus() asynchronous event demultiplexing function to dequeue the next completion event from the completion port:
      BOOL status = GetQueuedCompletionStatus
         (completion_port_, &num_bytes,
         0, &act,
         wait_time == 0 ? 0 : wait_time->msec ());


When this function returns, the ACT received from the Windows NT operating system is downcast to become an Async_Result *:
      Async_Result *async_result =
         static_cast <Async_Result *> (act);


The completion event that GetQueuedCompletionStatus() returned updates the completion result data members in async_result:
      async_result->status (status);
      if (!status)
         async_result->error (GetLastError ());
      else
         async_result->bytes_transferred(num_bytes);


The proactor implementation’s handle_events() method then invokes the complete() method on the async_result adapter:
      async_result->complete ();


Implementation activity 5.4 (240) illustrates how the complete() method in the Async_Stream_Read_Result adapter dispatches to the concrete completion handler’s handle_event() hook method.

Finally, the proactor deletes the async_result pointer, which was allocated dynamically by an asynchronous operation interface method, as shown in implementation activity 3.1 (232).
         delete async_result;
      }


The private portion of our proactor implementation caches the handle to its Windows NT completion port:
private:
   // Store a HANDLE to a Windows NT completion port.
   HANDLE completion_port_;
};



6 Determine the number of proactors in an application. Many applications can be structured using just one instance of the Proactor pattern. In this case the proactor can be implemented using the Singleton pattern [GoF95], as shown in implementation activity 4 (235). This design is useful for centralizing event demultiplexing and dispatching of completion events to a single location in an application. 

It can be useful to run multiple proactors simultaneously within the same application process, however. For example, different proactors can be associated with threads running at different priorities. This design provides different quality of service levels to process completion handlers for asynchronous operations.

Note that completion handlers are only serialized per thread within an instance of the proactor. Multiple completion handlers in multiple threads can therefore run in parallel. This configuration may necessitate the use of additional synchronization mechanisms if completion handlers in different threads access shared state concurrently. Mutexes and synchronization idioms such as Scoped Locking (325) are suitable.


7 Implement the concrete completion handlers. Concrete completion handlers specialize the completion handler interface described in implementation activity 2.3 (228) to define application-specific functionality. Three sub-activities must be addressed when implementing concrete completion handlers: 

7.1 Determine policies for maintaining state in concrete completion handlers. A concrete completion handler may need to maintain state information associated with a particular request. For example, an operating system may notify a server that only part of a file was written to a Socket asynchronously, due to the occurrence of transport-level flow control. A concrete completion handler must then send the remaining data, until the file is fully transferred or the connection becomes invalid. It must therefore know which file was originally specified, how many bytes remain to be sent, and the position of the file at the start of the previous request. 

7.2 Select a mechanism to configure concrete completion handlers with a handle. Concrete completion handlers perform operations on handles. The same two strategies described in implementation activity 6.2 of the Reactor (179) pattern—hard-coded and generic—can be applied to configure handles with event handlers in the Proactor pattern. In both strategies wrapper facades (47) can encapsulate handles used by completion handler classes. 

7.3 Implement completion handler functionality. Application developers must decide the processing actions that should be performed to implement a service when its corresponding hook method is invoked by a proactor. To separate connection establishment functionality from subsequent service processing, concrete completion handlers can be divided into several categories in accordance with the Acceptor-Connector pattern (285). In particular, service handlers implement application-specific services. In contrast, acceptors and connectors establish connections passively and actively, respectively, on behalf of these service handlers. 

8 Implement the initiators. In many proactive applications, such as our Web server example, the concrete completion handlers are the initiators. In this case this implementation activity can be skipped. Initiators that are not completion handlers, however, are often used to initiate asynchronous service processing during an application’s start-up phase. 

Example Resolved

Our Web server uses Windows NT features, such as overlapped I/O, completion ports, and GetQueuedCompletionStatus(), to implement proactive event demultiplexing. It employs a single-method completion handler dispatch interface strategy that can process multiple Web browser service requests asynchronously. HTTP acceptors asynchronously connect and create HTTP handlers using a variant of the Acceptor-Connector pattern (285). Each HTTP handler is responsible for asynchronously receiving, processing, and replying to a Web browser GET request delivered to the Web server’s proactor via a completion event. The example shown here uses a single thread to invoke asynchronous operations and handle completion event processing. It is straightforward to enhance this example to take advantage of multiple threads, however, as described in the Variants section.

The Web server’s main() function starts by performing its initialization activities, such as creating a proactor singleton, a Windows NT completion port, and an HTTP acceptor. This acceptor associates its passive-mode acceptor handle with the proactor singleton’s completion port. The Web server next performs the following scenario during its connection processing:

 
•The Web server invokes the HTTP acceptor’s accept() method (1). This method creates an ACT containing itself as the concrete completion handler.
•Acting in the role of an initiator, the HTTP acceptor’s accept() method then invokes the Win32 AcceptEx() operation asynchronously. It passes the ACT to AcceptEx(), together with a HANDLE that identifies both the passive-mode socket endpoint to accept connections and the completion port that Windows NT8 should use to queue the completion event when AcceptEx() finishes accepting a connection.
•The Web server’s main() function then invokes the proactor’s (3) handle_events() method. This method runs the proactor’s event loop, which calls the GetQueuedCompletionStatus() asynchronous event demultiplexer. This function waits on its completion port for the operating system to queue completion events when asynchronous operations finish executing.
•A remote Web browser subsequently connects to the Web server (4), which causes the asynchronous AcceptEx() operation to accept the connection and generate an accept completion event. The operating system then locates this operation’s ACT and associates it with the completion event. At this point it queues the updated completion event on the appropriate completion port (5).
•The GetQueuedCompletionStatus() function running in the application’s event loop thread then dequeues the completion event from the completion port. The proactor uses the ACT associated with this completion event to dispatch the handle_event() hook method on the HTTP acceptor completion handler (6), passing it the ACCEPT_EVENT event type.
•To process the completion event, the HTTP acceptor creates an HTTP handler (7) that associates its I/O handle with the proactor’s completion port. This HTTP handler then immediately invokes an asynchronous ReadFile() operation (8) to obtain the GET request data sent by the Web browser. The HTTP handler passes itself as the completion handler in the ACT to ReadFile() together with the I/O handle. The operating system uses the completion port associated with this handle to notify the proactor’s handle_events() method when the asynchronous ReadFile() operation finishes executing.
•Control of the Web server then returns to the proactor’s event loop (9), which calls the GetQueuedCompletionStatus() function to continue waiting for completion events.

After the connection is established and the HTTP handler is created, the following diagram illustrates the subsequent scenario used by a proactive Web server to service an HTTP GET request:

 
•The Web browser sends an HTTP GET request (1).
•The asynchronous ReadFile() operation invoked in the previous scenario then finishes executing and the operating system queues the read completion event onto the completion port (2). This event is then dequeued by GetQueuedCompletionStatus(), which returns to the proactor’s handle_events() method. This method demultiplexes the completion event’s ACT to the designated HTTP handler and dispatches the handler’s handle_event() hook method, passing the READ_EVENT event type (3).
•The HTTP handler parses the request (4). Steps (2) through (4) then repeat as necessary until the entire GET request has been received asynchronously.
•After the GET request has been completely received and validated, the HTTP handler memory-maps the requested file (5) and invokes an asynchronous WriteFile() operation to transfer the file data via the connection (6). The HTTP handler passes an ACT that identifies itself as a completion handler to WriteFile(), so that the proactor can notify it after the asynchronous WriteFile() operation finishes.
•After the asynchronous WriteFile() operation finishes the operating system inserts a write completion event into the completion port. The proactor uses GetQueuedCompletionStatus() again to dequeue the completion event (7). It uses its associated ACT to demultiplex to the HTTP handler, then dispatches its handle_event() hook method (8) to process the write completion event results. Steps (6) through (8) continue asynchronously until the entire file has been delivered to the Web browser.

Below we illustrate how the HTTP handler in our Web server can be written using the Completion_Handler class defined in the Implementation section.
class HTTP_Handler : public Completion_Handler {
   // Implements HTTP using asynchronous operations.


HTTP_Handler inherits from the ‘single-method’ dispatch interface variant of the Completion_Handler base class defined in implementation activity 2.3 (228). This design enables the proactor singleton to dispatch its handle_events() hook method when asynchronous ReadFile() and WriteFile() operations finish. The following data members are contained in each HTTP_Handler object:
private:
   // Cached <Proactor>.
   Proactor *proactor_;
   // Memory-mapped file_;
   Mem_Map file_;
   // Socket endpoint, initialized into “async-mode.”
   SOCK_Stream *sock_;
   // Hold the HTTP Request while its being processed.
   HTTP_Request request_;
   // Read/write asynchronous socket I/O.
   Async_Stream stream_;


The constructor caches a pointer to the proactor used by the HTTP_Handler:
public:
   HTTP_Handler (Proactor *proactor):
      proactor_ (proactor) { }


When a Web browser connects to the Web server the following open() method of the HTTP handler is called by the HTTP acceptor:
   virtual void open (SOCK_Stream *sock) {
      // Initialize state for request.
      request_.state_ = INCOMPLETE;
 
      // Store pointer to the socket.
      sock_ = sock;
 
      // Initialize <Async_Stream>.
      stream_.open
         (this, // This completion handler.
         sock_->handle (), proactor_);
 
      // Start asynchronous read operation on socket.
      stream_.async_read
       (request_.buffer (), request_.buffer_size ());
   }


In open(), the Async_Stream is initialized with the completion handler, handle, and proactor to use when asynchronous ReadFile() and WriteFile() operations finish. It then invokes an async_read() operation and returns to the proactor that dispatched it. When the call stack unwinds the Web server will continue running its handle_events() event loop method on its proactor singleton.

After the asynchronous ReadFile() operation completes, the proactor singleton demultiplexes to the HTTP_Handler completion handler and dispatches its subsequent handle_event() method:
   virtual void handle_event
      (HANDLE,
      Event_Type event_type,
      const Async_Result &async_result) {
   if (event_type == READ_EVENT) {
         if (!request_.done
            (async_result.bytes_transferred ()))
            // Didn’t get entire request, so start a
            // new asynchronous read operation.
            stream_.async_read (request_.buffer (),
                  request_.buffer_size ());
         else
            parse_request ();
         }
         // …
      }


If the entire request has not arrived, another asynchronous ReadFile() operation is invoked and the Web server returns once again to its event loop. After a complete GET request has been received from a Web browser, however, the following parse_request() method maps the requested file into memory and writes the file data to the Web browser asynchronously:
void parse_request () {
   // Switch on the HTTP command type.
   switch (request_.command ()) {
 
   // Web browser is requesting a file.
   case HTTP_Request::GET:
      // Memory map the requested file.
      file_.map (request_.filename ());
      // Invoke asynchronous write operation.
      stream_.async_write (file_.buffer (),
                     file_.buffer_size ());
      break;
   // Web browser is storing file at the Web server.
   case HTTP_Request::PUT:
      // …
   }
}


This sample implementation of parse_request() uses a C++ switch statement for simplicity and clarity. A more extensible implementation could apply the Command pattern [GoF95] or Command Processor pattern [POSA1] instead.

When the asynchronous WriteFile() operation completes, the proactor singleton dispatches the handle_event() hook method of the HTTP_Handler:
virtual void handle_event
   (HANDLE, Event_Type event_type,
   const Async_Result &async_result) {
   // … see READ_EVENT case above …
   else if (event_type == WRITE_EVENT) {
      if (!file_.done
         (async_result.bytes_transferred ()))
         // Didn’t send entire data, so start
         // another asynchronous write.
         stream_.async_write
         (file_.buffer (),file_.buffer_size ());
      else
         // Success, so free up resources…
   }
}


After all the data has been received the HTTP handler frees resources that were allocated dynamically.

The Web server contains a main() function that implements a single-threaded server. This server first calls an asynchronous accept operation and the waits in the proactor singleton’s handle_events() event loop:
// HTTP server port number.
const u_short PORT = 80;
 
int main () {
   // HTTP server address.
   INET_Addr addr (PORT);
 
   // Initialize HTTP server endpoint, which associates
   // the <HTTP_Acceptor>’s passive-mode socket handle
   // with the <Proactor> singleton’s completion port.
   HTTP_Acceptor acceptor (addr, Proactor::instance ());
 
   // Invoke an asynchronous <accept> operation to
   // Invoke the Web server processing.
   acceptor.accept ();
 
   // Event loop processes client connection requests
   // and HTTP requests proactively.
   for (;;)
      Proactor::instance ()->handle_events ();
   /* NOTREACHED */
}


As service requests arrive from Web browsers and are converted into indication events by the operating system, the proactor singleton invokes the event handling hook methods on the HTTP_Acceptor and HTTP_Handler concrete event handlers to accept connections and receive and process logging records asynchronously. The sequence diagram below illustrates the behavior in the proactive Web server.

The proactive processing model shown in this diagram can scale when multiple HTTP handlers and HTTP acceptors process requests from remote Web browsers simultaneously. For example, each handler/acceptor can invoke asynchronous ReadFile(), WriteFile(), and AcceptEx() operations that run concurrently. If the underlying asynchronous operation processor supports asynchronous I/O operations efficiently the overall performance of the Web server will scale accordingly.

 

Variants

Asynchronous Completion Handlers. The Implementation section describes activities used to implement a proactor that dispatches completion events to completion handlers within a single proactor event loop thread. When a concrete completion handler is dispatched, it borrows the proactor’s thread to perform its completion processing. However, this design may restrict the concrete completion handler to perform short-duration synchronous processing to avoid decreasing the overall responsiveness of the application significantly.

To resolve this restriction, all completion handlers could be required to act as initiators and invoke long-duration asynchronous operations immediately, rather than performing the completion processing synchronously. Some operating systems, such as Windows NT, explicitly support asynchronous procedure calls (APCs). An APC is a function that executes asynchronously in the context of its calling thread. When an APC is invoked the operating system queues it within the thread context. The next time the thread is idle, such as when it blocks on an I/O operation, it can run the queued APCs.

Concurrent Asynchronous Event Demultiplexer. One downside to using APCs is that they may not use multiple CPUs effectively. This is because each APC runs in a single thread context. A more scalable strategy therefore may be to create a pool of threads that share an asynchronous event demultiplexer, so that a proactor can demultiplex and dispatch completion handlers concurrently. This strategy is particularly scalable on operating system platforms that implement asynchronous I/O efficiently.

For example, a Windows NT completion port [Sol98] is optimized to run efficiently when accessed by GetQueuedCompletionStatus() from multiple threads simultaneously [HPS99]. In particular, the Windows NT kernel schedules threads waiting on a completion port in ‘last-in first-out’ (LIFO) order. This LIFO protocol maximizes CPU cache affinity [Mog95] by ensuring that the thread waiting the shortest time is scheduled first, which is an example of the Fresh Work Before Stale pattern [Mes96].

Shared Completion Handlers. Iinitiators can invoke multiple asynchronous operations simultaneously, all of which share the same concrete completion handler [ARSK00]. To behave correctly, however, each shared handler may need to determine unambiguously which asynchronous operation has completed. In this case, the initiator and proactor must collaborate to shepherd operation-specific state information throughout the entire asynchronous processing life-cycle.

As with implementation activity 3.1 (232), the Asynchronous Completion Token pattern (261) can be re-applied to disambiguate each asynchronous operation—an initiator can create an asynchronous completion token (ACT) that identifies each asynchronous operation uniquely. It then ‘piggy-backs’ this initiator-ACT onto the ACT passed when an asynchronous operation is invoked on an asynchronous operation processor. When the operation finishes executing and is being processed by the proactor, the ‘initiator-ACT’ can be passed unchanged to the shared concrete completion handler’s hook method. This initiator-ACT allows the concrete completion handler to control its subsequent processing after it receives an asynchronous operation’s completion results.

 To share a concrete completion handler we first add an initiator-ACT data member and a pair of set/get methods to the Async_Result class:
class Async_Result : public OVERLAPPED {
private:
   const void *initiator_act_;
   // ….
public:
   // Set/get initiator’s ACT.
   void initiator_act (const void *);
   const void *initiator_act ();
   // …


We next modify the Async_Stream I/O methods to ‘piggy-back’ the initiator-ACT with its existing ACT:
int Async_Stream::async_read (void *buf,
                  u_long n_bytes,
                  const void *initiator_act)
{
   u_long bytes_read;
   OVERLAPPED *act = new // Create the ACT.
      Async_Stream_Read_Result (completion_handler_);
 
   // Set <initiator_act> in existing ACT.
   act->initiator_act (initiator_act);
 
   ReadFile (handle_, buf, n_bytes, &bytes_read, act);
}


Finally, we can retrieve this initiator-ACT in a concrete event handler’s handle_event() method via the Async_Result parameter:
virtual void handle_event
         (HANDLE, Event_Type event_type,
         const Async_Result &async_result) {
   const void *initiator_act =
      async_result.initiator_act ();
   // …
}


The handle_event() method can use this initiator_act to disambiguate its subsequent processing.

Asynchronous Operation Processor Emulation. Many operating system platforms, including the traditional versions of UNIX [MBKQ96] and the Java Virtual Machine (JVM), do not export asynchronous operations to applications. There are several techniques that can be used to emulate an asynchronous operation processor on such platforms, however. A common solution is to employ a concurrency mechanism to execute operations without blocking initiators, such as the Active Object pattern (369) or some type of threading model. Three activities must be addressed when implementing a multi-threaded asynchronous operation processor:
•Operation invocation. When an operation is invoked the asynchronous operation processor must first store its associated ACT in an internal table. This can be implemented using the Manager pattern [Som97].
•Asynchronous operation execution. The operation will next be executed in a different thread of control than the invoking initiator thread. One strategy is to spawn a thread for each operation. A more scalable strategy is for the asynchronous operation processor to maintain a pool of threads using the Active Object pattern (369) Thread Pool variant. This strategy requires the initiator thread to queue the operation request before continuing with its other computations. 
Each operation will subsequently be dequeued and executed in a thread internal to the asynchronous operation processor. For example, to implement asynchronous read operations an internal thread can block while reading from socket or file handles. Operations thus appear to execute asynchronously to initiators that invoke them, even though the operations block internally within the asynchronous operation processor in their own thread of control.

•Operation completion handling. When an asynchronous operation completes the asynchronous operation processor generates a completion event and associates it with the appropriate ACT it had cached during the original invocation. It then queues the updated completion event into the appropriate completion event queue.

Other variants. Several variants of the Proactor pattern are similar to variants in the Reactor pattern (179), such as integrating the demultiplexing of timer and I/O events, and supporting concurrent concrete completion handlers.

Known uses

Completion ports in Windows NT. The Windows NT operating system provides the mechanisms to implement the Proactor pattern efficiently [Sol98]. Various asynchronous operations are supported by Windows NT, such as time-outs, accepting new network connections, reading and writing to files and Sockets, and transmitting entire files across a Socket connection. The operating system itself is thus the asynchronous operation processor. Results of the operations are queued as completion events on Windows NT completion ports, which are then dequeued and dispatched by an application-provided proactor.

The POSIX AIO family of asynchronous I/O operations. On some real-time POSIX platforms the Proactor pattern is implemented by the aio_*() family of APIs [POSIX95]. These operating system features are similar to those described above for Windows NT. One difference is that UNIX signals can be used to implement a pre-emptively asynchronous proactor in which a signal handler can interrupt an application’s thread of control. In contrast, the Windows NT API is not pre-emptively asynchronous, because application threads are not interrupted. Instead, the asynchronous completion routines are called back at well-defined Win32 function points.

ACE Proactor Framework. The ADAPTIVE Communication Environment (ACE) [Sch97] provides a portable object-oriented Proactor framework that encapsulates the overlapped I/O and completion port mechanisms on Windows NT and the aio_*() family of asynchronous I/O APIs on POSIX platforms. ACE provides an abstraction class, ACE_Proactor, that defines a common interface to a variety of proactor implementations, such as ACE_Win32_Proactor and ACE_POSIX_Proactor. These proactor implementations can be created using different asynchronous event demultiplexers, such as GetQueuedCompletionStatus() and aio_suspend(), respectively.

Operating system device driver interrupt-handling mechanisms. The Proactor pattern is often used to enhance the structure of operating system kernels that invoke I/O operations on hardware devices driven by asynchronous interrupts. For example, a packet of data can be written from an application to a kernel-resident device driver, which then passes it to the hardware device that transmits the data asynchronously. When the device finishes its transmission it generates a hardware interrupt that notifies the appropriate handler in the device driver. The device driver then processes the interrupt to completion, potentially initiating another asynchronous transfer if more data is available from the application.

Phone call initiation via voice mail. A real-life application of the Proactor pattern is the scenario in which you telephone a friend, who is currently away from her phone, but who returns calls reliably when she comes home. You therefore leave a message on her voice mail to ask her to call you back. In terms of the Proactor pattern, you are a initiator who invokes an asynchronous operation on an asynchronous operation processor—your friend’s voice mail—to inform your friend that you called. While waiting for your friend’s ‘call-back’ you can do other things, such as re-read chapters in POSA2. After your friend has listened to her voice mail, which corresponds to the completion of the asynchronous operation, she plays the proactor role and calls you back. While talking with her, you are the completion handler that ‘processes’ her ‘callback’.

Consequences

The Proactor pattern offers a variety of benefits:

Separation of concerns. The Proactor pattern decouples application-independent asynchronous mechanisms from application-specific functionality. The application-independent mechanisms become reusable components that know how to demultiplex the completion events associated with asynchronous operations and dispatch the appropriate callback methods defined by concrete completion handlers. Similarly, the application-specific functionality in concrete completion handlers know how to perform particular types of service, such as HTTP processing.

Portability. The Proactor pattern improves application portability by allowing its interface to be reused independently of the underlying operating system calls that perform event demultiplexing. These system calls detect and report the events that may occur simultaneously on multiple event sources. Event sources may include I/O ports, timers, synchronization objects, signals, and so on. For example, on real-time POSIX platforms the asynchronous I/O functions are provided by the aio_*() family of APIs [POSIX95]. Similarly, on Windows NT, completion ports and overlapped I/O are used to implement asynchronous I/O [MDS96].

Encapsulation of concurrency mechanisms. A benefit of decoupling the proactor from the asynchronous operation processor is that applications can configure proactors with various concurrency strategies without affecting other application components and services.

Decoupling of threading from concurrency. The asynchronous operation processor executes potentially long-duration operations on behalf of initiators. Applications therefore do not need to spawn many threads to increase concurrency. This allows an application to vary its concurrency policy independently of its threading policy. For instance, a Web server may only want to allot one thread per CPU, but may want to service a higher number of clients simultaneously via asynchronous I/O.

Performance. Multi-threaded operating systems use context switching to cycle through multiple threads of control. While the time to perform a context switch remains fairly constant, the total time to cycle through a large number of threads can degrade application performance significantly if the operating system switches context to an idle thread.9 For example, threads may poll the operating system for completion status, which is inefficient. The Proactor pattern can avoid the cost of context switching by activating only those logical threads of control that have events to process. If no GET request is pending, for example, a Web server need not activate an HTTP Handler.

Simplification of application synchronization. As long as concrete completion handlers do not spawn additional threads of control, application logic can be written with little or no concern for synchronization issues. Concrete completion handlers can be written as if they existed in a conventional single-threaded environment. For example, a Web server’s HTTP handler can access the disk through an asynchronous operation, such as the Windows NT TransmitFile() function [HPS99], hence no additional threads need to be spawned.

The Proactor pattern has the following liabilities:

Restricted applicability. The Proactor pattern can be applied most efficiently if the operating system supports asynchronous operations natively. If the operating system does not provide this support, however, it is possible to emulate the semantics of the Proactor pattern using multiple threads within the proactor implementation. This can be achieved, for example, by allocating a pool of threads to process asynchronous operations. This design is not as efficient as native operating system support, however, because it increases synchronization and context switching overhead without necessarily enhancing application-level parallelism.

Complexity of programming, debugging and testing. It is hard to program applications and higher-level system services using asynchrony mechanisms, due to the separation in time and space between operation invocation and completion. Similarly, operations are not necessarily constrained to run at well-defined points in the processing sequence—they may execute in non-deterministic orderings that are hard for many developers to understand.

Applications written with the Proactor pattern can also be hard to debug and test because the inverted flow of control oscillates between the proactive framework infrastructure and the method callbacks on application-specific handlers. This increases the difficulty of ‘single-stepping’ through the run-time behavior of a framework within a debugger, because application developers may not understand or have access to the proactive framework code.

Scheduling, controlling, and canceling asynchronously running operations. Initiators may be unable to control the scheduling order in which asynchronous operations are executed by an asynchronous operation processor. If possible, therefore, an asynchronous operation processor should employ the Strategy pattern [GoF95] to allow initiators to prioritize and cancel asynchronous operations. Devising a completely reliable and efficient means of canceling all asynchronous operations is hard, however, because asynchronous operations may complete before they can be cancelled.

See Also

The Proactor pattern is related to the Observer [GoF95] and Publisher-Subscriber [POSA1] patterns, in which all dependents are informed when a single subject changes. In the Proactor pattern, however, completion handlers are informed automatically when completion events from multiple sources occur. In general, the Proactor pattern is used to demultiplex multiple sources of asynchronously delivered completion events to their associated completion handlers, whereas an observer or subscriber is usually associated with a single source of events.

The Proactor pattern can be considered an asynchronous variant of the synchronous Reactor pattern (179). The Reactor pattern is responsible for demultiplexing and dispatching multiple event handlers that are triggered when it is possible to invoke an operation synchronously without blocking. In contrast, the Proactor pattern supports the demultiplexing and dispatching of multiple completion handlers that are triggered by the completion of operations that execute asynchronously.

Leader/Followers (447) and Half-Sync/Half-Async (423) are two other patterns that demultiplex and process various types of events synchronously. On platforms that support asynchronous I/O efficiently, the Proactor pattern can often be implemented more efficiently than these patterns. However, the Proactor pattern may be harder to implement because it has more participants, which require more effort to understand. The Proactor’s combination of ‘inversion of control’ and asynchrony may also require application developers to have more experience to use and debug it effectively.

The Active Object pattern (369) decouples method execution from method invocation. The Proactor pattern is similar, because an asynchronous operation processor performs operations asynchronously on behalf of initiators. Both patterns can therefore be used to implement asynchronous operations. The Proactor pattern is often used instead of the Active Object pattern on operating systems that support asynchronous I/O efficiently.

The Chain of Responsibility [GoF95] pattern decouples event handlers from event sources. The Proactor pattern is similar in its segregation of initiators and completion handlers. In the Chain of Responsibility pattern, however, the event source has no prior knowledge of which handler will be executed, if any. In Proactor, initiators have full control over the target completion handler. The two patterns can be combined by establishing a completion handler that is the entry point into a responsibility chain dynamically configured by an external factory.

Current Java implementations do not support Proactor-like event processing schemes, because java.io does not support asynchronous I/O. In basic Java implementations blocking I/O operations can even block the whole Java Virtual Machine (JVM)—the I/O operation blocks the current thread and, as multi-threading may be implemented in user space, the operating system considers the task running the JVM as blocked and schedules other operating system processes instead of other JVM threads.

More sophisticated Java implementations work around this problem by implementing asynchronous I/O internally on the native code level—the thread doing the blocking call is blocked, but other threads are able to run. The blocked thread is subsequently called back, or may explicitly wait for the blocking call to return. Applications cannot make use of this directly, however, because current JDK libraries do not expose asynchronous I/O. This will change with the next generation of the Java I/O system, which is under development and will appear as a package called java.nio or something similar [JSR51].

Certain programming languages, such as Scheme, support continuations. Continuations can be used in single-threaded programs to enable a sequence of function calls to relinquish its run-time call stack when blocked without losing the execution history of the call stack. In the context of the Proactor pattern, the indirect transfer of control from an asynchronous operation invocation to the subsequent processing by its completion handler can be modeled as a continuation.

Credits

Tim Harrison, Thomas D. Jordan, and Irfan Pyarali are co-authors of the original version of the Proactor pattern. Irfan also provided helpful comments on this version. Thanks to Ralph Johnson for suggestions that helped improve this pattern and for pointing out how this pattern relates to the programming language feature continuations.



Safari Books Online

Create Bookmark (Key: b)


Create Note or Tag (Key: t)


Email This Page (Key: e)


Print



Zoom Out (Key: -)


Zoom In (Key: +)


Toggle to Full Screen (Key: f)


Previous (Key: p)


Next (Key: n)





